{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37efed94-3bac-47bc-a2ef-0df586ec5e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "loaded_model = joblib.load('sentiment_analysis_model.pkl')\n",
    "\n",
    "# Now you can use loaded_model for prediction\n",
    "test_headline = [\"One in five BEST buses now electric make up 1.6% of all\"]\n",
    "predicted_label = loaded_model.predict(test_headline)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a682e-766b-4d82-acba-050a634b914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sidhesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sidhesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sidhesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the topic:  amul\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function for preprocessing\n",
    "def preprocessing(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_negative_articles(topic):\n",
    "    # Scrape news articles\n",
    "    url = f'https://news.google.com/search?q={topic}&hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    articles = soup.find_all('a', class_='WwrzSb')\n",
    "    articles_list = []\n",
    "\n",
    "    for article in articles:\n",
    "        link = 'https://news.google.com' + article['href'][1:]\n",
    "        try:\n",
    "            p1 = requests.get(link)\n",
    "            if p1 is None or p1.status_code in [403, 404]:\n",
    "                continue\n",
    "\n",
    "            soup1 = BeautifulSoup(p1.text, 'html.parser')\n",
    "            heading_tag = soup1.find('h1')\n",
    "\n",
    "            if heading_tag:\n",
    "                heading = heading_tag.get_text(strip=True)\n",
    "                text = '\\n'.join([p.get_text(strip=True) for p in soup1.find_all('p')])\n",
    "                articles_list.append((heading, text))\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Connection error occurred: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save scraped articles in a CSV file\n",
    "    file_name = f\"{topic}_articles.csv\"\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Heading', 'Text'])\n",
    "        writer.writerows(articles_list)\n",
    "\n",
    "    # Preprocess scraped articles\n",
    "    news = pd.DataFrame(articles_list, columns=['Heading', 'Text'])\n",
    "    news['Heading'] = news['Heading'].apply(preprocessing)\n",
    "\n",
    "    # Drop rows with NaN in the 'Text' column\n",
    "    news.dropna(subset=['Heading'], inplace=True)\n",
    "\n",
    "    # Load sentiment analysis model and tokenizer\n",
    "    #loaded_model = load_model('sentiment_analysis_model.h5')\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(news['Heading'])\n",
    "\n",
    "    # Pad sequences\n",
    "    sequences = tokenizer.texts_to_sequences(news['Heading'])\n",
    "    maxlen = 100\n",
    "    #padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "    # Predict sentiments\n",
    "    predictions = loaded_model.predict(news['Heading'])\n",
    "\n",
    "    # Filter out articles with negative sentiment\n",
    "    negative_articles = news[predictions == 0]\n",
    "\n",
    "    return negative_articles\n",
    "\n",
    "# Example usage\n",
    "topic = input(\"Enter the topic: \")\n",
    "negative_articles = get_negative_articles(topic)\n",
    "print(\"Negative Articles:\")\n",
    "print(negative_articles[['Heading']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00626df2-6a4c-488e-a8c3-a298a71a5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from tkinter import messagebox\n",
    "from PIL import Image, ImageTk\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function for preprocessing\n",
    "def preprocessing(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def get_negative_articles(topic):\n",
    "    # Scrape news articles\n",
    "    url = f'https://news.google.com/search?q={topic}&hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    articles = soup.find_all('a', class_='WwrzSb')\n",
    "    articles_list = []\n",
    "\n",
    "    for article in articles:\n",
    "        link = 'https://news.google.com' + article['href'][1:]\n",
    "        try:\n",
    "            p1 = requests.get(link)\n",
    "            if p1 is None or p1.status_code in [403, 404]:\n",
    "                continue\n",
    "\n",
    "            soup1 = BeautifulSoup(p1.text, 'html.parser')\n",
    "            heading_tag = soup1.find('h1')\n",
    "\n",
    "            if heading_tag:\n",
    "                heading = heading_tag.get_text(strip=True)\n",
    "                text = '\\n'.join([p.get_text(strip=True) for p in soup1.find_all('p')])\n",
    "                articles_list.append((heading, text))\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Connection error occurred: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save scraped articles in a CSV file\n",
    "    file_name = f\"{topic}_articles.csv\"\n",
    "    with open(file_name, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Heading', 'Text'])\n",
    "        writer.writerows(articles_list)\n",
    "\n",
    "    # Preprocess scraped articles\n",
    "    news = pd.DataFrame(articles_list, columns=['Heading', 'Text'])\n",
    "    news['Heading'] = news['Heading'].apply(preprocessing)\n",
    "\n",
    "    # Drop rows with NaN in the 'Text' column\n",
    "    news.dropna(subset=['Heading'], inplace=True)\n",
    "\n",
    "    # Load sentiment analysis model and tokenizer\n",
    "    #loaded_model = load_model('sentiment_analysis_model.h5')\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(news['Heading'])\n",
    "\n",
    "    # Pad sequences\n",
    "    sequences = tokenizer.texts_to_sequences(news['Heading'])\n",
    "    maxlen = 100\n",
    "    #padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "    # Predict sentiments\n",
    "    #predictions = loaded_model.predict(news['Heading'])\n",
    "\n",
    "    # For demonstration, assuming all articles are negative\n",
    "    predictions = [0] * len(news)\n",
    "\n",
    "    # Filter out articles with negative sentiment\n",
    "    negative_articles = news[predictions == 0]\n",
    "\n",
    "    return negative_articles\n",
    "\n",
    "def search_topic():\n",
    "    topic = topic_entry.get()\n",
    "    if topic:\n",
    "        negative_articles = get_negative_articles(topic)\n",
    "        if not negative_articles.empty:\n",
    "            display_output(negative_articles)\n",
    "        else:\n",
    "            messagebox.showinfo(\"Info\", \"No negative articles found for the given topic.\")\n",
    "    else:\n",
    "        messagebox.showerror(\"Error\", \"Please enter a topic.\")\n",
    "\n",
    "def display_output(articles):\n",
    "    output_text.config(state=tk.NORMAL)\n",
    "    output_text.delete('1.0', tk.END)\n",
    "    for index, row in articles.iterrows():\n",
    "        output_text.insert(tk.END, f\"{row['Heading']}\\n\\n\")\n",
    "    output_text.config(state=tk.DISABLED)\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Negative News Finder\")\n",
    "root.geometry(\"800x600\")\n",
    "\n",
    "# Add background image\n",
    "background_image = Image.open(\"background_image.jpg\")\n",
    "background_image = background_image.resize((800, 600), Image.ANTIALIAS)\n",
    "bg_image = ImageTk.PhotoImage(background_image)\n",
    "background_label = tk.Label(root, image=bg_image)\n",
    "background_label.place(relwidth=1, relheight=1)\n",
    "\n",
    "# Add title label\n",
    "title_label = tk.Label(root, text=\"Negative News Finder\", font=(\"Helvetica\", 24), bg=\"#007acc\", fg=\"white\")\n",
    "title_label.pack(fill=tk.X)\n",
    "\n",
    "# Add topic entry\n",
    "topic_entry = tk.Entry(root, font=(\"Helvetica\", 14))\n",
    "topic_entry.pack(pady=20, padx=20, fill=tk.X)\n",
    "\n",
    "# Add search button\n",
    "search_button = tk.Button(root, text=\"Search\", command=search_topic, font=(\"Helvetica\", 14))\n",
    "search_button.pack(pady=10)\n",
    "\n",
    "# Add output text area\n",
    "output_text = scrolledtext.ScrolledText(root, wrap=tk.WORD, font=(\"Helvetica\", 12), state=tk.DISABLED)\n",
    "output_text.pack(pady=20, padx=20, fill=tk.BOTH, expand=True)\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
